---
title: "<br><br>Text Modeling"
subtitle: "Using Tidy Data Principles"
author: "<br>Julia Silge | IBM Community Day: AI"
output:
  xaringan::moon_reader:
    css: ["robot-fonts", "default", "css/stack_orange.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    seal: false
    includes:
      in_header: header.html
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      dpi = 300, cache.lazy = FALSE,
                      fig.cap = "")
theme_set(silgelib::theme_roboto())
```

layout: true
  
<div class="my-footer"><span>bit.ly/silge-ibm-ai-day</span></div> 

---

BUMPER CHART HERE

---

class: center, middle

background-image: url(figs/white_title.svg)
background-size: cover

<img src="figs/so-logo.svg" width="30%"/>

# Text Modeling
## Using Tidy Data Principles

### Julia Silge | IBM Community Day: AI

---
class: left, middle

background-image: url(figs/white_bg.svg)
background-size: cover

<img src="figs/so-icon.svg" width="15%"/>

## Find me at...

<a href="http://twitter.com/juliasilge"><i class="fa fa-twitter fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="http://github.com/juliasilge"><i class="fa fa-github fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="https://juliasilge.com"><i class="fa fa-link fa-fw"></i>&nbsp; juliasilge.com</a><br>
<a href="https://tidytextmining.com"><i class="fa fa-book fa-fw"></i>&nbsp; tidytextmining.com</a><br>
<a href="mailto:julia.silge@gmail.com"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; julia.silge@gmail.com</a>

---


background-image: url(figs/white_bg.svg)
background-size: cover

# Text in the real world

--

- Text data is increasingly important `r emo::ji("books")`

--

- NLP training is scarce on the ground `r emo::ji("scream")`

---

background-image: url(figs/cant_even.gif)
background-position: 50% 50%
background-size: 650px

---

background-image: url(figs/vexing.gif)
background-position: 50% 50%
background-size: 650px

---

class: center, middle

background-image: url(figs/tidytext.png)
background-size: 500px

---

class: center, middle

background-image: url(figs/tidytext_repo.png)
background-size: 900px

---

class: center, middle

background-image: url(figs/cover.png)
background-size: 450px

---

background-image: url(figs/white_title.svg)
background-size: cover

# Two powerful NLP modeling approaches

--

- Topic modeling

--

- Text classification

---

background-image: url(figs/white_bg.svg)
background-size: cover

# Topic modeling

- Each document = mixture of topics

--

- Each topic = mixture of words

---

background-image: url(figs/top_tags-1.png)
background-size: 800px

---

## great library heist `r emo::ji("sleuth")`

---

## Downloading your text data

```{r}
library(tidyverse)
library(gutenbergr)

titles <- c("Twenty Thousand Leagues under the Sea", 
            "The War of the Worlds",
            "Pride and Prejudice", 
            "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
    gutenberg_download(meta_fields = "title")

books
```

---

## Someone has torn your books apart! `r emo::ji("sob")`


```{r}
by_chapter <- books %>%
    group_by(title) %>%
    mutate(chapter = cumsum(str_detect(text, 
                                       regex("^chapter ", 
                                             ignore_case = TRUE)))) %>%
    ungroup() %>%
    filter(chapter > 0) %>%
    unite(document, title, chapter)

by_chapter
```

---

## Can we use topic modeling to put the books back together?

```{r}
library(tidytext)

word_counts <- by_chapter %>%
    unnest_tokens(word, text) %>%
    anti_join(get_stopwords(source = "smart")) %>%
    count(document, word, sort = TRUE)

word_counts

```

---

## Can we use topic modeling to put the books back together?

```{r}
words_sparse <- word_counts %>%
    cast_sparse(document, word, n)

class(words_sparse)
```

---

## Train a topic model

Use a sparse matrix or a `quanteda::dfm` object as input

```{r}
library(stm)

topic_model <- stm(words_sparse, K = 4, 
                   verbose = FALSE, init.type = "Spectral")

summary(topic_model)
```

---

## Exploring the output of topic modeling

Time for tidying!

```{r}
chapter_topics <- tidy(topic_model, matrix = "beta")

chapter_topics
```

---

## Exploring the output of topic modeling

```{r}
top_terms <- chapter_topics %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)

top_terms
```

---
## Exploring the output of topic modeling

```{r, eval=FALSE}
top_terms %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip()
```

---

```{r, echo=FALSE}
library(drlib)

top_terms %>%
    ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    coord_flip() +
    scale_x_reordered() +
    labs(y = expression(beta), x = NULL)
```

---

## How are documents classified?

```{r}
chapters_gamma <- tidy(topic_model, matrix = "gamma",
                       document_names = rownames(words_sparse))

chapters_gamma
```

---

## How are documents classified?

```{r}
chapters_parsed <- chapters_gamma %>%
    separate(document, c("title", "chapter"), sep = "_", convert = TRUE)

chapters_parsed
```

---

## How are documents classified?

```{r, eval=FALSE}
chapters_parsed %>%
    mutate(title = reorder(title, gamma * topic)) %>%
    ggplot(aes(factor(topic), gamma)) +
    geom_boxplot() +
    facet_wrap(~ title)
```

---

```{r, echo=FALSE}
chapters_parsed %>%
    mutate(title = reorder(title, gamma * topic)) %>%
    ggplot(aes(factor(topic), gamma, color = factor(topic))) +
    geom_boxplot(show.legend = FALSE) +
    facet_wrap(~ title) +
    labs(x = "Topic", y = expression(gamma))
```

---

# going farther `r emo::ji("rocket")`

## Which words in each document are assigned to which topics?

- `augment()`
- Add information to each observation in the original data

---

background-image: url(figs/stm_video.png)
background-size: 850px

---

## Using stm

- Document-level covariates

```{r, eval=FALSE}
topic_model <- stm(words_sparse, K = 0, init.type = "Spectral",
                   prevalence = ~s(Year),
                   data = covariates,
                   verbose = FALSE)
```

- Use functions for `semanticCoherence()`, `checkResiduals()`, `exclusivity()`, and more!
- Check out http://www.structuraltopicmodel.com/
- See my recent blog post for how to choose `K`, the number of topics

---

## Using stm

INCLUDE PLOT FROM BLOG POST

---

background-image: url(figs/white_title.svg)
background-size: cover

# Stemming? 

Advice from [Schofield & Mimno](https://mimno.infosci.cornell.edu/papers/schofield_tacl_2016.pdf)

"Comparing Apples to Apple: The Effects of Stemmers on Topic Models"

---

class: right, middle

<h1 class="fa fa-quote-left fa-fw"></h1>

<h2> Despite their frequent use in topic modeling, we find that stemmers produce no meaningful improvement in likelihood and coherence and in fact can degrade topic stability. </h2>

<h1 class="fa fa-quote-right fa-fw"></h1>

---

class: right, middle

background-image: url(figs/white_title.svg)
background-size: cover


# Text classification
<h1 class="fa fa-balance-scale fa-fw"></h1>

---

## Downloading your text data

```{r}
library(tidyverse)
library(gutenbergr)

titles <- c("The War of the Worlds",
            "Pride and Prejudice")

books <- gutenberg_works(title %in% titles) %>%
    gutenberg_download(meta_fields = "title") %>%
    mutate(document = row_number())

books
```

---

## Making a tidy dataset

Use this kind of data structure for EDA! `r emo::ji("nail")`

```{r}
library(tidytext)

tidy_books <- books %>%
    unnest_tokens(word, text) %>%
    group_by(word) %>%
    filter(n() > 50) %>%
    ungroup
```

---

## Cast to a sparse matrix

And build a dataframe with a response variable

```{r}
sparse_words <- tidy_books %>%
    count(document, word, sort = TRUE) %>%
    cast_sparse(document, word, n)

books_joined <- data_frame(document = as.integer(rownames(sparse_words))) %>%
    left_join(books %>%
                  select(document, title))
```

---

## Train a glmnet model

```{r}
library(glmnet)
library(doMC)
registerDoMC(cores = 8)

is_jane <- books_joined$title == "Pride and Prejudice"

model <- cv.glmnet(sparse_words, is_jane, family = "binomial", 
                   parallel = TRUE, keep = TRUE)
```

---

## Tidying our model

Tidy, and then filter to choose some lambda from glmnet output

```{r}
library(broom)

coefs <- model$glmnet.fit %>%
    tidy() %>%
    filter(lambda == model$lambda.1se)

Intercept <- coefs %>%
    filter(term == "(Intercept)") %>%
    pull(estimate)
```

---

## Tidying our model

```{r}
classifications <- tidy_books %>%
    inner_join(coefs, by = c("word" = "term")) %>%
    group_by(document) %>%
    summarize(Score = sum(estimate)) %>%
    mutate(Probability = plogis(Intercept + Score))

classifications
```

---

## Understanding our model

```{r, eval=FALSE}
coefs %>%
    group_by(estimate > 0) %>%
    top_n(15, abs(estimate)) %>%
    ungroup %>%
    ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
    geom_col(show.legend = FALSE) +
    coord_flip()
```

---

```{r, echo = FALSE}
coefs %>%
    group_by(estimate > 0) %>%
    top_n(15, abs(estimate)) %>%
    ungroup %>%
    ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
    geom_col(show.legend = FALSE) +
    coord_flip() +
    labs(x = NULL,
         title = "Coefficients that increase/decrease probability of classification",
         subtitle = "A document mentioning Martians is unlikely to be written by Jane Austen")
```

---

## ROC

```{r}
comment_classes <- classifications %>%
    left_join(books %>%
                  select(title, document), by = "document") %>%
    mutate(Correct = case_when(title == "Pride and Prejudice" ~ TRUE,
                               TRUE ~ FALSE))

roc <- comment_classes %>%
    arrange(desc(Probability)) %>%
    mutate(TPR = cumsum(Correct) / sum(Correct),
           FPR = cumsum(!Correct) / sum(!Correct),
           FDR = cummean(!Correct))
```

---

## ROC

```{r}
roc %>%
    arrange(Probability)
```

---

```{r, echo = FALSE}
roc %>%
    arrange(Probability) %>%
    ggplot(aes(FPR, TPR)) +
    geom_abline(lty = 2, alpha = 0.5, 
                color = "gray50",
                size = 1.5) + 
    geom_line(color = "midnightblue",
              size = 1.8) +
    labs(title = "ROC for text classification")
```

---

## AUC for model

```{r}
roc %>%
    summarise(AUC = sum(diff(FPR) * na.omit(lead(TPR) + TPR)) / 2)
```

---

## Misclassifications

Let's talk about misclassifications. Which documents here were incorrectly predicted to be written by Jane Austen?

```{r}
roc %>%
    filter(Probability > .8, !Correct) %>%
    sample_n(10) %>%
    inner_join(books %>%
                   select(document, text)) %>%
    select(Probability, text)
```

---

## Misclassifications

Let's talk about misclassifications. Which documents here were incorrectly predicted to *not* be written by Jane Austen?

```{r}
roc %>%
    filter(Probability < .2, Correct) %>%
    sample_n(10) %>%
    inner_join(books %>%
                   select(document, text)) %>%
    select(Probability, text)
```

---

## Workflow for text mining/modeling

PUT FLOW CHART HERE

---
background-image: url(figs/lizzieskipping.gif)
background-position: 50% 50%
background-size: 750px

# Go explore real-world text!

---

class: left, middle

background-image: url(figs/white_bg.svg)
background-size: cover

<img src="figs/so-icon.svg" width="15%"/>

# Thanks!

<a href="http://twitter.com/juliasilge"><i class="fa fa-twitter fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="http://github.com/juliasilge"><i class="fa fa-github fa-fw"></i>&nbsp; @juliasilge</a><br>
<a href="https://juliasilge.com"><i class="fa fa-link fa-fw"></i>&nbsp; juliasilge.com</a><br>
<a href="https://tidytextmining.com"><i class="fa fa-book fa-fw"></i>&nbsp; tidytextmining.com</a><br>
<a href="mailto:julia.silge@gmail.com"><i class="fa fa-paper-plane fa-fw"></i>&nbsp; julia.silge@gmail.com</a>

Slides created with [remark.js](http://remarkjs.com/) and the R package [**xaringan**](https://github.com/yihui/xaringan)

---

PUT BUMPER CHART HERE

